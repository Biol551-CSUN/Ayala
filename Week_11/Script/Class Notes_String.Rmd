---
title: "Working with Words"
author: "Nicole Ayala"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE, 
                      messages = FALSE)
```

```{r}
library(stopwords)
library(tidytext)
library(janeaustenr)
library(wordcloud2)
library(tidyverse)
library(here)
```

### Intro to String
- What is a string?
  - a character that is within quotations
```{r}
words<-"This is a string" # a string given a name
words_vector<-c("Apples", "Bananas","Oranges") # several strings within a vector
```
  
There are 4 basic families of functions in the {stringr} package:

1. Manipulation: these functions allow you to manipulate individual characters within the strings in character vectors.
2. Whitespace tools to add, remove, and manipulate whitespace.
3. Locale sensitive operations whose operations will vary from locale to locale.
4. Pattern matching functions. These recognize four engines of pattern description. The most common is regular expressions, but there are three other tools.

### Manipulation

```{r}
paste("High temp", "Low pH") #Paste words together. This can be useful if say you have a two columns of treatments and you want to combine them into one (e.g., high temp, low temp and high pH, low pH).
paste("High temp", "Low pH", sep = "-") # put a separater you want
paste0("High temp", "Low pH") # removes the space between the two columns
shapes <- c("Square", "Circle", "Triangle") # types of shapes
paste("My favorite shape is a", shapes) #in every statement has 3 possible shapes 
two_cities <- c("best", "worst") # This is very useful when making labels for your plots
paste("It was the", two_cities, "of times.")
```
### Manipulation: Indiividual characters
```{r}
shapes # vector of shapes
str_length(shapes) # how many letters are in each word?
seq_data<-c("ATCCCGTC")
str_sub(seq_data, start = 2, end = 4) # extract the 2nd to 4th AA
str_sub(seq_data, start = 3, end = 3) <- "A" # replaces 3rd with an A in  (modify the strings) CCC to ACC
seq_data
#str_sub(seq_data, start = 3, end = 4) <- "AT" # replaces an A and T in the 3rd and 4th position (modify the strings)
#seq_data
str_dup(seq_data, times = c(2, 3)) # times is the number of times to duplicate each string
```
### Whitespace
- Say you have a column and you did not copy and paste your treatments like you learned in the first week of class. You now have some words with extra white spaces and R thinks its an entirely new word. Here is how to deal with that...

```{r}
badtreatments<-c("High", " High", "High ", "Low", "Low")
badtreatments 
str_trim(badtreatments) # this removes any whitespace on either side but not the  middle
str_trim(badtreatments, side = "left") # this removes left
str_pad(badtreatments, 5, side = "right") # add a white space to the right side after the 5th character
str_pad(badtreatments, 5, side = "right", pad = "1") # add a 1 to the right side after the 5th character but it will onyl add psace tot he fifth position - if there are 3 letters 4 and 4 will be whitespaced
```
### Locale Sensitive 
- Important, these will perform differently in different places in the world/with different languages. The default is English, but you can set the language setting.

```{r}
x<-"I love R!"
str_to_upper(x) # Make everything upper case
str_to_lower(x) # Make it lower case
str_to_title(x) # Make it title case (Cap first letter of each word)
```
### Pattern matching
-stringr} has functions to view, detect, locate, extract, match, replace, and split strings based on specific patterns.

```{r}
 data<-c("AAA", "TATA", "CTAG", "GCTT")# View a specific pattern in a vector of strings
str_view(data, pattern = "A") # find all the strings with an A
str_view(data, pattern = "G") # find all the strings with an G
str_detect(data, pattern = "A") # detect a specific pattern aka A
str_detect(data, pattern = "AT") # which ones have AT
str_locate(data, pattern = "AT") # locate a pattern
```
### Regex: regular expressions
- There are several types of regular expressions:
* Metacharacters: The simplest form of regular expressions are those that match a single character. Most characters, including all letters and digits, are regular expressions that match themselves. For a language like R, there are some special characters that have reserved meaning and they are referred to as ‘Metacharacters”. Additional meaning beyond human language. The metacharacters in Extended Regular Expressions (EREs) are:

. \ | ( ) [ { $ * + ?
* Sequences: as the name suggests refers to the sequences of characters which can match. We have shorthand versions (or anchors) for commonly used sequences in R:
* Quantifiers
* Character classes
* POSIX character classes (Portable Operating System Interface)
```{r}
vals<-c("a.b", "b.c","c.d") # set of strings
str_replace(vals, "\\.", " ") # string, pattern, replace all the "." with a space
```

### Metacharacter: Aside About the Functions
- Each function in {stringr} has two forms a basic form that searches for the first instance of a character and a *_all that searches for all instances. 
```{r}
vals<-c("a.b.c", "b.c.d","c.d.e") # has multiple .
str_replace(vals, "\\.", " ") # string, pattern, replace
str_replace_all(vals, "\\.", " ") # string, pattern, replace and str_replace only replaces the first instance
```

### Sequences
- subset the vector to only keep strings with digits
```{r}
val2<-c("test 123", "test 456", "test")
str_subset(val2, "\\d")
```
### Character Class
- A character class or character set is a list of characters enclosed by square brackets [ ]. Character sets are used to match only one of the different characters. For example, the regex character class [aA] matches any lower case letter a or any upper case letter A
```{r}
str_count(val2, "[aeiou]") # count the number of lowercase vowels in each string
str_count(val2, "[0-9]") # count any digit
```
### Quantifiers
- Example - find the phone numbers
- Make a regex that finds all the strings that contain a phone number. We know there is a specific pattern (3 numbers, 3 numbers, 4 numbers and it can have either a "." or "-" to separate them). Let's also say we know that the first number cannot be a 1
```{r}
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})" # {2} means any 2nd one of the previous one, separator can be a [-.]

```
```{r}
strings<-c("123.4567.75",
         "444.5694.92",
         "126.4971.34",
         "apple",
         "home: 562.3458.92")
phone <- "([0-9]{3}) [.] ([0-9]{4}) [.] ([0-9]{2})"
```

```{r}
#test<-str_subset(strings, phone) %>% # subset only the strings with phone numbers
#test_fix<-test %>%
#str_replace_all("\\.", replacement = "-") %>% # replace periods with -
#str_replace_all("\\D", replacement = " ") %>%
#str_trim() %>%
#str_replace_all(" ", replacement = "-"")
#test
```
```{r}
#test %>%
#  str_replace_all(pattern = "\\.", replacement = "-") %>% # replace periods with -
#  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% # remove all the things we don't want
# str_trim() # trim the white space
```
### Tidytext
- Package for text mining and making text tidy. This is very helpful for social sciences or anyone that uses survey data. Also, really helpful for text mining abstracts to write a review paper on a topic.
- DO NOT USE VIEW!!!!
-Because we are interest in text mining, we will want to clean this so that there is only one word per row so its tidy. In tidytext each word is refered to as a token. The function to unnest the data so that its only one word per row is unnest_tokens().
-OK so we now have >735,000 rows of words.... but, some of these words are kind of useless. Words that are common and don't really have important meaning (e.g. "and","by","therefore"...). These are called stopwords. We can use the function "get_stopwords()" to essentially remove these words from our dataframe. (This function is essentially just a dataframe of unnecessary words)
```{r}
head(austen_books()) # explore it 
tail(austen_books())
original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>%
  mutate(line = row_number(), # find every line
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", # count the chapters (starts with the word chapter followed by a digit or roman numeral)
                                                 ignore_case = TRUE)))) %>% #ignore lower or uppercase
  ungroup() # ungroup it so we have a dataframe again
# don't try to view the entire thing... its >73000 lines...
head(original_books)
tidy_books <- original_books %>%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column
head(tidy_books) # there are now >725,000 rows. Don't view the entire thing!
cleaned_books <- tidy_books %>%
  anti_join(get_stopwords()) # dataframe without the stopwords
#see an example of all the stopwords
head(get_stopwords())
head(cleaned_books) # by = join_by(world)
cleaned_books %>%
  count(word, sort = TRUE)

```
How would we modify this code to count the most popular words by book? What about by each chapter within a book? --> sentiment analysis
- sentiment analysis
There are many ways that we can now analyze this tidy dataset of text. One example is we could do a sentiment analysis (how many positive and negative words) using get_sentiments(). An important note: I was not an English major and I know there are many different lexicons, but I know nothing about them. Look at the help files if you want to go deeper into this...

```{r}
sent_word_counts <- tidy_books %>%
  inner_join(get_sentiments()) %>% # only keep pos or negative words
  count(word, sentiment, sort = TRUE) # count them

```
- Now, think about how we could do this with science? Instead of get_sentiments(), you could use an inner_join with a vector of keywords that you are searching for.
```{r}
sent_word_counts %>%
  filter(n > 150) %>% # take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %>% # sort it so it gows from largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

Make a Wordcloud
```{r}
words<-cleaned_books %>%
  count(word) %>% # count all the words
  arrange(desc(n))%>% # sort the words
  slice(1:100) #take the top 100
wordcloud2(words, shape = 'triangle', size=0.3) # make a wordcloud out of the top 100 words
```
